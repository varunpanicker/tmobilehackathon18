{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "23ef262e-29d6-4e8b-997c-04b41a673809",
    "_uuid": "cadc8c2b841d66f6b3c6de673707711c33591b06"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will explore a customer segmentation and their result. As there is plenty of Kernel with a clear explaination of the cleanup, I'll just do it quickly without a lot of information. More explanation will be given on the preparation of the dataset and the result parts. For reading purpose, some codes will be hidden to avoid having a dataset extremely long. Feel free to let me know you thinking about it and I hope you gonna enjoy it !\n",
    "\n",
    "# Summary\n",
    "\n",
    "The Notebook is divided in 5 parts as follow:\n",
    "\n",
    "<ul>\n",
    "<li>Pre-Processing</li>\n",
    "<ul>\n",
    "<li>Classic Cleanup</li>\n",
    "<li>Customer Country</li>\n",
    "</ul>\n",
    "<li>Data Preparation on Articles</li>\n",
    "<ul>\n",
    "<li>Preparation of Matrices</li>\n",
    "<li>Clustering</li>\n",
    "<li>Analysis</li>\n",
    "</ul>\n",
    "<li>New Datasets per Invoice</li>\n",
    "<li>New Dataset per Customer</li>\n",
    "<ul>\n",
    "<li>Preparation for Clustering</li>\n",
    "<li>Clustering</li>\n",
    "<li>Analysis</li>\n",
    "</ul>\n",
    "<li>Building Model</li>\n",
    "</ul>\n",
    "\n",
    "Now we can start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5a661e5-563a-4863-a501-ef37a4c08075",
    "_uuid": "6bba590200a1d658d7d26a92206fea117d292f8e"
   },
   "source": [
    "# Pre-Processing \n",
    "\n",
    "### Classic Cleanup\n",
    "\n",
    "In this part, I'll handle Cancelled Orders, remove all descriptions which means that the object is lost and so on. All invoces from the customer will be removed as well as Shipping costs (StockCode = POST or DOT). A new feature for the Price of each line will be created and some outliers (in term of Unit Price or Qty) will be remove if and only if there is a cancellation existing. There is for example 40k article bought and cancelled just after with the same article/customer. Without cancellation, I'll consider that it's not a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/data.csv\", encoding=\"ISO-8859-1\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "0031e123-221a-4c3f-9c83-1b6ab24813e4",
    "_uuid": "5d8f2f7de2197d186630606957b56909492889ae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handle cancellation as new feature\n",
    "df[\"Cancelled\"] = df[\"InvoiceNo\"].str.startswith(\"C\")\n",
    "df[\"Cancelled\"] = df[\"Cancelled\"].fillna(False)\n",
    "\n",
    "# Hnadle incorrect Description\n",
    "df = df[df[\"Description\"].str.startswith(\"?\") == False]\n",
    "df = df[df[\"Description\"].str.isupper() == True]\n",
    "df = df[df[\"Description\"].str.contains(\"LOST\") == False]\n",
    "df = df[df[\"CustomerID\"].notnull()]\n",
    "df[\"CustomerID\"] = df[\"CustomerID\"].astype(int)\n",
    "\n",
    "# Convert Invoice Number to integer as we already consider Cancellation as new feature\n",
    "df['InvoiceNo'].replace(to_replace=\"\\D+\", value=r\"\", regex=True, inplace=True)\n",
    "df['InvoiceNo'] = df['InvoiceNo'].astype('int')\n",
    "\n",
    "# remove shiping invoices\n",
    "df = df[(df[\"StockCode\"] != \"DOT\") & (df[\"StockCode\"] != \"POST\")]\n",
    "df.drop(\"StockCode\", inplace=True, axis=1)\n",
    "\n",
    "# remove outliers by qty\n",
    "qte_false = [74215, 3114, 80995]  # fond during exploration but not done here (found with a boxplot on qty or price)\n",
    "for qte in qte_false:\n",
    "    df = df[(df[\"Cancelled\"] == False) & (df[\"Quantity\"] !=qte)]\n",
    "\n",
    "# Now we can only keep the order without cancellation\n",
    "df = df[df[\"Cancelled\"] == False]\n",
    "df.drop(\"Cancelled\", axis=1, inplace=True)\n",
    "\n",
    "# We can create the feature Price\n",
    "df[\"Price\"] = df[\"UnitPrice\"] * df[\"Quantity\"]\n",
    "\n",
    "# convert date to proper datetime\n",
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "99624fe8-797e-4105-b9d0-774e74b5c511",
    "_uuid": "090286b4332a66fd47811849e5491afbedb7791f"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5e0e773-bb5b-4864-85cf-8d80af816265",
    "_uuid": "708ac9a9718bc2bd793777399fc724489820233e"
   },
   "source": [
    "Now the dataset starts to be clean, we can now convert our Countries to a value. To do so, we will rank them based on their impact of the revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b50e8c36-7c35-4708-871b-19575d0ddbab",
    "_uuid": "c09c98d639a62a3a12d61ff1fac73d834f4c0a68",
    "collapsed": true
   },
   "source": [
    "### Handling Customer Country\n",
    "\n",
    "As mentionned above, here we can think about encoding Country to their rank based on their impact on the Revenue. Let's look at values for number of invoices per country and the revenue per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "8445d014-0019-4886-a90a-f683739ad5d1",
    "_uuid": "6a4fd66112504a4723194ccac2f2dde4d730c64f"
   },
   "outputs": [],
   "source": [
    "revenue_per_countries = df.groupby([\"Country\"])[\"Price\"].sum().sort_values()\n",
    "revenue_per_countries.plot(kind='barh', figsize=(15,12))\n",
    "plt.title(\"Revenue per Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1c1b362-de2e-4cb6-a948-0a6aa1963e52",
    "_uuid": "17699e99f87d70b9a2ba128c70a472cc27198feb"
   },
   "outputs": [],
   "source": [
    "No_invoice_per_country = df.groupby([\"Country\"])[\"InvoiceNo\"].count().sort_values()\n",
    "No_invoice_per_country.plot(kind='barh', figsize=(15,12))\n",
    "plt.title(\"Number of Invoices per Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ebc3bfea-e8c4-434d-b78c-b22951b0cbf0",
    "_uuid": "a9ace5ba6c25eb60a8b5f865f64cf8f892735c47"
   },
   "source": [
    "We can see that for example Netherland bring more money with less Invoices. That means we may order country by their average purchase price instead of only the price or the quantity. Now the strategy changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "7421e48e-8d91-49ed-a434-28b0280317cc",
    "_uuid": "e090fb7a616d1499a99417593e0e9581ad1af189"
   },
   "outputs": [],
   "source": [
    "best_buyer = df.groupby([\"Country\", \"InvoiceNo\"])[\"Price\"].sum().reset_index().groupby([\"Country\"])[\"Price\"].mean().sort_values()\n",
    "best_buyer.plot(kind='barh', figsize=(15,12))\n",
    "plt.title(\"Average Basket Price per Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a8b30f1-7d14-4d05-ac72-66a74884c4be",
    "_uuid": "983f15c57e2fae4d0a074ff1c4a14a05788d6fb3"
   },
   "source": [
    "That's it, we now have a cleaner scale as the country won't be biaised by the huge number of order comming from UK. It's also a good indicator on where the Shop should expand his market. If I were him, I'd think about Netherlands ! Now let's encode the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "ab672dab-cd13-4d38-817b-32132b49e713",
    "_uuid": "7c02fb6fdb6182036b6a4792b3bd94dd68a4e673"
   },
   "outputs": [],
   "source": [
    "encoder_countries = best_buyer.rank().to_dict()\n",
    "decoder_countries = {i: j for i, j in encoder_countries.items()}\n",
    "\n",
    "df[\"Country\"]  = df[\"Country\"].apply(lambda x:encoder_countries[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e4ecc5a3-6dd1-495e-b639-5b73eab40c58",
    "_uuid": "8c20fca18335af0da05ed96488fc397c78a0f30e",
    "collapsed": true
   },
   "source": [
    "# Preparation of articles\n",
    "\n",
    "Now we have only 1 feature remaining to pre-process before to go on, it's the articles descriptions. To do so, we will first generate a Term-Frequency Matrix and a Term-Frequency Invert Document-Frequency Matrix with all uniques articles. Then we will prepare this matrix to be able to run a clustering on it. The clustering will be tried with multiple size of clusters. The best parameter will be used an analysis of the content will be done in order to check that it's relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "16ea4cad-ba91-42a3-ba68-c5ba606eb907",
    "_uuid": "dcf5c851044b18fafca4ad5fcd5b6af780907bf7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[\"Description\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "80eec202-a3ae-4880-b572-64f4a9717987",
    "_uuid": "e9e9448b9e992a55cdbe673447ce4d8c657c08fc"
   },
   "outputs": [],
   "source": [
    "X = df[\"Description\"].unique()\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def stem_and_filter(doc):\n",
    "    tokens = [stemmer.stem(w) for w in analyzer(doc)]\n",
    "    return [token for token in tokens if token.isalpha()]\n",
    "\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "CV = CountVectorizer(lowercase=True, stop_words=\"english\", analyzer=stem_and_filter)\n",
    "TF_matrix = CV.fit_transform(X)\n",
    "print(\"TF_matrix :\", TF_matrix.shape, \"of\", TF_matrix.dtype)\n",
    "\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "CV = TfidfVectorizer(lowercase=True, stop_words=\"english\", analyzer=stem_and_filter, min_df=0.00, max_df=0.3)  # we remove words if it appears in more than 30 % of the corpus (not found stopwords like Box, Christmas and so on)\n",
    "TF_IDF_matrix = CV.fit_transform(X)\n",
    "print(\"TF_IDF_matrix :\", TF_IDF_matrix.shape, \"of\", TF_IDF_matrix.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "100bf3f3-e716-46c6-8e7f-03121f7077af",
    "_uuid": "99ddbd9109a82a9d3eabf798618772599889953e"
   },
   "source": [
    "So there is no word which appears in more than a third of our dataset. We can now think about which matrix should we use ? Both matrices are very sparse so before to do the clustering, we should ensure that we will be able to compute proper distances. The thing to do in such case, it's to go throught the TruncatedSVD. I used 100 output features but we may increase it. 100 is the default value to perform what is called Latent Semantic Analysis. Both matrices will be full of float64 numbers. After Normalization to have a norm of 1 for each row, we can do a clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "7e288223-38f9-4127-bfb7-9e437c82bea6",
    "_uuid": "36bd29bebef13911669c52bee95e0f80d414eea6"
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 100)\n",
    "normalizer = Normalizer(copy=False)\n",
    "\n",
    "TF_embedded = svd.fit_transform(TF_matrix)\n",
    "TF_embedded = normalizer.fit_transform(TF_embedded)\n",
    "print(\"TF_embedded :\", TF_embedded.shape, \"of\", TF_embedded.dtype)\n",
    "\n",
    "TF_IDF_embedded = svd.fit_transform(TF_IDF_matrix)\n",
    "TF_IDF_embedded = normalizer.fit_transform(TF_IDF_embedded)\n",
    "print(\"TF_IDF_embedded :\", TF_IDF_embedded.shape, \"of\", TF_IDF_embedded.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "284dbc76-cce0-4f13-b946-8edc06563de0",
    "_uuid": "870c024ba87bc0857821382b41a9b445c32b08b0"
   },
   "source": [
    "For the clustering, we can check the Silhouette Score. If we do that with multiple number of cluster we will see that there is a flat after 60 clusters. To dertermine the best choice after that, we can also take a look at the number of articles in both clusters. If all articles are in 1 cluters, that lean the clustering is very poor. We can also take a look at the distribution of article per clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e6ed1f6-327c-4e6b-b58b-1739273c919a",
    "_uuid": "38b7550ffdd1198fa93a0a33067618b571e06737",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_tf = []\n",
    "score_tfidf = []\n",
    "mean_tf = []\n",
    "std_tf = []\n",
    "mean_tfidf = []\n",
    "std_tfidf = []\n",
    "\n",
    "x = list(range(5, 105, 5))\n",
    "\n",
    "for n_clusters in x:\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n",
    "    kmeans.fit(TF_embedded)\n",
    "    clusters = kmeans.predict(TF_embedded)\n",
    "    silhouette_avg = silhouette_score(TF_embedded, clusters)\n",
    "#     print(\"N clusters =\", n_clusters, \"Silhouette Score :\", silhouette_avg)\n",
    "    rep = np.histogram(clusters, bins = n_clusters-1)[0]\n",
    "    score_tf.append(silhouette_avg)\n",
    "    mean_tf.append(rep.mean())\n",
    "    std_tf.append(rep.std())\n",
    "\n",
    "for n_clusters in x:\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n",
    "    kmeans.fit(TF_IDF_embedded)\n",
    "    clusters = kmeans.predict(TF_IDF_embedded)\n",
    "    silhouette_avg = silhouette_score(TF_IDF_embedded, clusters)\n",
    "#     print(\"N clusters =\", n_clusters, \"Silhouette Score :\", silhouette_avg)\n",
    "    rep = np.histogram(clusters, bins = n_clusters-1)[0]\n",
    "    score_tfidf.append(silhouette_avg)\n",
    "    mean_tfidf.append(rep.mean())\n",
    "    std_tfidf.append(rep.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "cb75848b-fb05-4ac5-ab89-4108590181cb",
    "_uuid": "69520fe25bc78c9675faf5bd48801f68db925c4c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,16))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x, score_tf, label=\"TF matrix\")\n",
    "plt.plot(x, score_tfidf, label=\"TF-IDF matrix\")\n",
    "plt.title(\"Evolution of the Silhouette Score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(x, mean_tfidf, label=\"TF-IDF mean\")\n",
    "plt.plot(x, mean_tf, label=\"TF mean\")\n",
    "plt.plot(x, std_tfidf, label=\"TF-IDF St.Dev\")\n",
    "plt.plot(x, std_tf, label=\"TF St.Dev\")\n",
    "plt.ylim(0, 200)\n",
    "plt.title(\"Evolution of the mean and Std of both clusters\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b259349b-1eb0-4512-89a4-ce9fde3fd4d0",
    "_uuid": "5f36133e5725b16b711272dfb834a9978c591a3e"
   },
   "source": [
    "We can see that 100 cluster is the value where the silhouette score is the highest and and Std Dev minimum. In term of choice, we should select the matrix TF-IDF instead of TF as the score is highest after 60 clusters. Let's do a final clustering with 100 cluster. and we can explore the content of some clusters by using cloudword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "_cell_guid": "9317c784-9808-42d9-a3e7-dbbf624d29bd",
    "_uuid": "85b92bf3b1e79cd81c430dafa7a094a8e54c6645",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 100\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30, random_state=42)\n",
    "kmeans.fit(TF_IDF_embedded)\n",
    "clusters = kmeans.predict(TF_IDF_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "_cell_guid": "69fea66f-64d6-4056-93e3-54cdb324fb62",
    "_uuid": "6f7a066a79f8bd7e5fbf3a65e3d0974da7e67142"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "wc = WordCloud()\n",
    "\n",
    "for num, cluster in enumerate(random.sample(range(100), 12)) :\n",
    "    plt.subplot(3, 4, num+1)\n",
    "    wc.generate(\" \".join(X[np.where(clusters==cluster)]))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title(\"Cluster {}\".format(cluster))\n",
    "    plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3c59386-3e6c-4d1a-a16a-fbe0f9cc61bc",
    "_uuid": "e39f6ac8451c053995ceb1d2ffaa7f13945d5584",
    "collapsed": true
   },
   "source": [
    "We can see that the content is different for both clusters and the balance is quite good from the calculation. We can consider it as a good clustering. We can now map the article to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "84b5e04d-4c8d-4b6e-ac03-fd0847d0b8d6",
    "_uuid": "32a78823ec8504284f327b90a4463afc7bd0aba8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_article_to_cluster = {article : cluster for article, cluster in zip(X, clusters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "618f6a2d-09ce-45ac-9473-ce61c783b7ad",
    "_uuid": "bab79d63405211afe1c1bab69413c8acd9778628"
   },
   "source": [
    "We can also take a look at the position of clusters in space with TSNE. A good alternative and faster is UMAP (https://github.com/lmcinnes/umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1f69c04e-6c04-48bb-a747-2dc3dba6df07",
    "_uuid": "17bc4c85df17a79951ef1fe652785963b6ad0f4d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "proj = tsne.fit_transform(TF_IDF_embedded)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(proj[:,0], proj[:,1], c=clusters)\n",
    "plt.title(\"Visualisation of the clustering with TSNE\", fontsize=\"25\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96c1467b-2615-4ce1-af76-82f4e3ce76b8",
    "_uuid": "cf4ed7ad46cdef6b6cf678b40885be2a99016768"
   },
   "source": [
    "# Modelization\n",
    "\n",
    "### Intermediate dataset grouped by invoices\n",
    "\n",
    "Now we have everything to start the preparation of the final dataset. First we will One-Hot-Encode the price spent on every cluster par invoices. In parralleel, we will also perform a groupby on the dataset initial before to merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "c7e0d9f0-17f7-4a81-90f7-0f709af5ad41",
    "_uuid": "36a8f16c40aa718b89bc59a5f51a5147d7e30155",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = df['Description'].apply(lambda x : dict_article_to_cluster[x])\n",
    "df2 = pd.get_dummies(cluster, prefix=\"Article_cluster\").mul(df[\"Price\"], 0)\n",
    "df2 = pd.concat([df['InvoiceNo'], df2], axis=1)\n",
    "df2_grouped = df2.groupby('InvoiceNo').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "8e0288b9-ed0b-4d97-8e45-08c354528dfe",
    "_uuid": "df4c811c8a5bd323c1bed49c1c6b6ab896c69824",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_aggregation = {}\n",
    "custom_aggregation[\"Price\"] = \"sum\"\n",
    "custom_aggregation[\"InvoiceDate\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"CustomerID\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"Country\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"Quantity\"] = \"sum\"\n",
    "\n",
    "df_grouped = df.groupby(\"InvoiceNo\").agg(custom_aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "4c890c09-cb29-4821-846a-cc361c675db9",
    "_uuid": "4a6ce9dbff56aa23e83ef9f6c7eb00c38c01671a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let create recency for every Invoice\n",
    "now = df_grouped[\"InvoiceDate\"].max()  # as the dataset is not done in the present\n",
    "df_grouped[\"Recency\"] = now - df_grouped[\"InvoiceDate\"]\n",
    "df_grouped[\"Recency\"] = pd.to_timedelta(df_grouped[\"Recency\"]).astype(\"timedelta64[D]\") # conversion to day from now\n",
    "\n",
    "# add features required for the next groupby\n",
    "df_grouped[\"nb_visit\"] = 1\n",
    "df_grouped[\"total_spent\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a2cd4d09-553a-49d1-8b06-bf43ebbb17da",
    "_uuid": "1e554b77017ab7d0826ebf862dd3ae31cf8dda12"
   },
   "source": [
    "### Final Dataset per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "2afb65ad-171f-426a-89ab-ec893ba337b9",
    "_uuid": "4015592003fc668c68df423b6b76bf149ce2a975",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_grouped_final = pd.concat([df_grouped['CustomerID'], df2_grouped], axis=1).set_index(\"CustomerID\").groupby(\"CustomerID\").sum()\n",
    "df2_grouped_final = df2_grouped_final.div(df2_grouped_final.sum(axis=1), axis=0)\n",
    "df2_grouped_final = df2_grouped_final.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "10ddefb0-be2a-436b-808f-a5e1ec07ace8",
    "_uuid": "1bb0b461f5782d9bd920d4182c1136b7e51c217e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_aggregation = {}\n",
    "custom_aggregation[\"Price\"] = [\"mean\", \"sum\"]\n",
    "custom_aggregation[\"nb_visit\"] = \"sum\"\n",
    "custom_aggregation[\"Country\"] = lambda x:x.iloc[0]\n",
    "custom_aggregation[\"Quantity\"] = \"sum\"\n",
    "custom_aggregation[\"Recency\"] = [\"min\", \"max\"]\n",
    "\n",
    "df_grouped_final = df_grouped.groupby(\"CustomerID\").agg(custom_aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "9f633e3c-0163-451d-a21d-a7d7c4baa9b1",
    "_uuid": "3e110ceb77b009f02f6cad7a65f9926c82fcd06d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_grouped_final[\"Freq\"] = (df_grouped_final[\"Recency\"][\"max\"]  - df_grouped_final[\"Recency\"][\"min\"] ) / df_grouped_final[\"nb_visit\"][\"sum\"]\n",
    "df_grouped_final.columns = [\"avg_price\", \"sum_price\", \"nb_visit\", \"country\", \"quantity\", \"min_recency\", \"max_recency\", \"freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "872a3089-2669-4ac6-9332-9ba00aac919e",
    "_uuid": "347e6a4587a2ffe69d944db376f11f8fce3ed89d"
   },
   "outputs": [],
   "source": [
    "df_grouped_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f57ea68a-009a-4f90-8a25-bde682b305be",
    "_uuid": "08f3b94387edc370307a46caf182a220df0d9b77"
   },
   "source": [
    "### Clustering\n",
    "\n",
    "Now we have our 2 datasets. we won't merge them as there is not the same scaling. The scaling has to be done upfront. One dataset has raw values and the other one is scaled by row to have the percentage on every cluster. Now we will do a clustering and check the number of clusters required to explain all kind of customer (nevertheless, in order to be able to explain every group, the number of cluster will be limited to 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "bdf3075c-1135-4185-820f-1af462b16d66",
    "_uuid": "0cec7e5b2c001ee7d18c0e043ca09d61584ba556",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = df_grouped_final.as_matrix()\n",
    "X2 = df2_grouped_final.as_matrix()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X1 = scaler.fit_transform(X1)\n",
    "X_final_std_scale = np.concatenate((X1, X2), axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X1 = scaler.fit_transform(X1)\n",
    "X_final_minmax_scale = np.concatenate((X1, X2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "9c3427dc-171b-4cf4-b0de-48fb1b039ead",
    "_uuid": "939a5e66f2fa3e7544695f2f9943ab39f77a1316"
   },
   "outputs": [],
   "source": [
    "x = list(range(2, 10))\n",
    "y_std = []\n",
    "y_minmax = []\n",
    "for n_clusters in x:\n",
    "    print(\"n_clusters =\", n_clusters)\n",
    "    \n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=10)\n",
    "    kmeans.fit(X_final_std_scale)\n",
    "    clusters = kmeans.predict(X_final_std_scale)\n",
    "    silhouette_avg = silhouette_score(X_final_std_scale, clusters)\n",
    "    y_std.append(silhouette_avg)\n",
    "    print(\"The average silhouette_score is :\", silhouette_avg, \"with Std Scaling\")\n",
    "    \n",
    "    kmeans.fit(X_final_minmax_scale)\n",
    "    clusters = kmeans.predict(X_final_minmax_scale)\n",
    "    silhouette_avg = silhouette_score(X_final_minmax_scale, clusters)\n",
    "    y_minmax.append(silhouette_avg)\n",
    "    print(\"The average silhouette_score is :\", silhouette_avg, \"with MinMax Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "f0c2c7f2-3686-4334-bc2f-8d9cea364aa9",
    "_uuid": "db79fd155667371722e7e286e2a9273d4b3b49ca"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.plot(x, y_std, label=\"Using Standard Scaling\")\n",
    "plt.plot(x, y_minmax, label=\"Using MinMax Scaling\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Nombre de Clusters\")\n",
    "plt.ylabel(\"Score de Silhouette\")\n",
    "plt.title(\"Impact du Scaling sur le Clustering\")\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7eb5a368-8b34-47c8-bc41-63b94ec902f2",
    "_uuid": "8bcfaff1ac4ea631c782d380302bcb36059383ef"
   },
   "source": [
    "At cluster = 2, the score is clearly high beacuse the cluster group nearly all customers, we should focus only on higher sizes. On it we can see that the Standard Scaling is clearly better. Regarding the score, the best number of cluster seems to be 6. We can take a look at the histogram just to ensure that there is a balance even if we may have few customer really differents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "_cell_guid": "c2d0c722-1d00-4b4d-a079-c8493270912d",
    "_uuid": "5fed319c04fbbea3d9fa59fdb22094c3266640cc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_clusters = 6, n_init=30, random_state=42)  # random state just to be able to provide cluster number durint analysis\n",
    "kmeans.fit(X_final_std_scale)\n",
    "clusters = kmeans.predict(X_final_std_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "_cell_guid": "3d1d391a-05ed-4cfe-8308-25c09b4fbc9b",
    "_uuid": "0bb8e3bb7c6e70348a38ced81b4d5b5c2ff17f7e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,8))\n",
    "n, bins, patches = plt.hist(clusters, bins=6) # arguments are passed to np.histogram\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.title(\"Number of Customer per cluster\")\n",
    "plt.xticks([rect.get_x()+ rect.get_width() / 2 for rect in patches], [\"Cluster {}\".format(x) for x in range(6)])\n",
    "\n",
    "for rect in patches:\n",
    "    y_value = rect.get_height()\n",
    "    x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "    space = 5\n",
    "    va = 'bottom'\n",
    "    label = str(int(y_value))\n",
    "    \n",
    "    plt.annotate(\n",
    "        label,                      # Use `label` as label\n",
    "        (x_value, y_value),         # Place label at end of the bar\n",
    "        xytext=(0, space),          # Vertically shift label by `space`\n",
    "        textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "        ha='center',                # Horizontally center label\n",
    "        va=va)                      # Vertically align label differently for\n",
    "                                    # positive and negative values.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc3a4a46-d964-4b60-b062-dd273a038386",
    "_uuid": "ea556fa34b803d063ee7cdea791588adb3faa7d3"
   },
   "source": [
    "We have 1 cluster really empty (#4) with 6 customers, 2 very low (#2 and #3) and 3 quite balance (#0, #1 and #5). Let's now explore them. The small cluster may be very good or bad customers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "86b40d4f-384b-42cb-a8fa-ceeded27a3ac",
    "_uuid": "cc5a709077c51456b22e77e1fa32ab0b0e73ac15"
   },
   "source": [
    "### Analysis\n",
    "\n",
    "We will now used raw datas or the dataset of percentage per cluster to explore all types of customers. Let's start with a visualisation with TSNE as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "98ae24cc-5497-4b0d-ad3b-03011eb228ef",
    "_uuid": "dfaa11f11cc1d0f49364303efbec28cffd55cb58",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "proj = tsne.fit_transform(X_final_std_scale)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(proj[:,0], proj[:,1], c=clusters)\n",
    "plt.title(\"Visualisation of the clustering with TSNE\", fontsize=\"25\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e665eed0-85f4-4932-bafa-ee3bda64cfce",
    "_uuid": "3c4d80e1f5d08b269f932bb45fb41d886bc4f927"
   },
   "source": [
    "We can also compare both cluster based on the frequency of purchase and the average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "_cell_guid": "246e6bb2-06cd-4556-b27b-cae10ad7fadc",
    "_uuid": "57e3ef5a24cbd3db9ace549384af4afaf5a7c82a"
   },
   "outputs": [],
   "source": [
    "df_grouped_final[\"cluster\"] = clusters\n",
    "df_grouped_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_cell_guid": "3fcbebe5-35b1-4e6a-b423-de8e47fccd3c",
    "_uuid": "5b4c2bae4b3f4f45534193ea5ebfca3ec6d7cd02"
   },
   "outputs": [],
   "source": [
    "# custom_aggregation = {}\n",
    "# custom_aggregation[\"avg_price\"] = \"mean\"\n",
    "# custom_aggregation[\"nb_visit\"] = \"mean\"\n",
    "# custom_aggregation[\"Country\"] = lambda x:x.iloc[0]\n",
    "# custom_aggregation[\"min_recency\"] = \"mean\"\n",
    "# custom_aggregation[\"max_recency\"] = [\"min\", \"max\"]\n",
    "\n",
    "# df_grouped_final = df_grouped.groupby(\"CustomerID\").agg(custom_aggregation)\n",
    "\n",
    "df_analysis = df_grouped_final.groupby(\"cluster\").mean()\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "_cell_guid": "d55112c3-31ea-45fa-9213-be8ac049d7f5",
    "_uuid": "777f1443ed88522e338bda1c87334e5836c8bcf2"
   },
   "outputs": [],
   "source": [
    "price = df_analysis[\"avg_price\"].values\n",
    "freq = df_analysis[\"freq\"].values\n",
    "visit = df_analysis[\"nb_visit\"].values\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.scatter(price, freq, s=visit*20)\n",
    "    \n",
    "for label, x, y in zip([\"Customer #{}\".format(x) for x in range(6)], price, freq):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-20, 20),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "plt.title(\"Panier Moyen et fréquence d'achat par Cluster\")\n",
    "plt.xlabel(\"Average Price per Invoice\")\n",
    "plt.ylabel(\"Time between 2 invoices\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_cell_guid": "b6b87ed7-c3ef-43fa-9fef-6cb6e733d9b8",
    "_uuid": "b2c472fa4610f6df7276c20a4f26cc141d53febb"
   },
   "outputs": [],
   "source": [
    "x_max = df_analysis[\"min_recency\"].values\n",
    "x_min = df_analysis[\"max_recency\"].values\n",
    "freq = df_analysis[\"freq\"].values\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "for i in range(6):\n",
    "    plt.plot([-x_min[i] , -x_max[i]], [i, i], linewidth=freq[i]/5)\n",
    "\n",
    "plt.xlim(-365, 0)\n",
    "plt.title(\"Balance of invoices per Customer Clusters\")\n",
    "plt.xlabel(\"Recency\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4ade91b-2918-4165-bcdb-95150acd8d4a",
    "_uuid": "75ea8906fc381fcb8d374a74ce2ca7bd4f0c7276"
   },
   "source": [
    "The previous plots highlight the differences between clusters. \n",
    "\n",
    "The cluster #4 is having a very high mean price (3600 € per basket) and very often (every 5 days). If we take a look at histogramme of number of customer, we can see that this small group of people should be considered as V.I.P. and should be handled like \"outliers\". In term of history, we can also see that it's old customers and still active so probably happy from our store.  \n",
    "\n",
    "In term of frequency, we can see that there is 2 clusters with also a good frequency but a small basket (#0 and #1). If we take a look at the \"history\", we can see that it's customer who already purchase few time and we were not able to convert them to frequent customer. Unfortunately, the 2 cluster group more that 2350 customers. They probably join the store for a specific discount. \n",
    "\n",
    "To finish, if we take a look at the cluster #5 grouping 1850 customers, we can see that they order not very often but in a correct quantity (360€ every 60 days). We should maintain the fidelity of this group as it represent also an important part of the Revenue.\n",
    "\n",
    "Now let see what every cluster is purchasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_cell_guid": "a86d36fc-b7b7-4722-9963-4514244f7196",
    "_uuid": "30f07a501fa8337abca6ee570adee09f81e386d7"
   },
   "outputs": [],
   "source": [
    "purchase_mean = df2_grouped_final.set_index(clusters).groupby(clusters).mean()\n",
    "purchase_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "_cell_guid": "aa8f4187-c730-4663-9171-74aa6fd31da8",
    "_uuid": "3a8afb8b94eb7dd11105c793f48b078329b831f7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "ax = plt.subplot(111, projection='polar')\n",
    "theta = 2 * np.pi * np.linspace(0, 1, 100)\n",
    "matrix = purchase_mean.as_matrix()\n",
    "\n",
    "for i in range(6):\n",
    "    r = matrix[i, :]\n",
    "    ax.plot(theta, r, label=\"Customer Group {}\".format(i))\n",
    "\n",
    "ax.set_xticklabels([\n",
    "    \"Cluster 0\", \"Cluster 12\", \"Cluster 25\", \"Cluster 38\", \n",
    "    \"Cluster 50\", \"Cluster 62\", \"Cluster 75\", \"Cluster 88\"\n",
    "])\n",
    "\n",
    "ax.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "ax.set_title(\"Cluster of article bought per type of customer\", va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ee0a019-cf05-4985-b9fb-e88f044f2364",
    "_uuid": "4f7d89d76cad4f1013664fdaafeb8e5c3d326a97",
    "collapsed": true
   },
   "source": [
    "We can see taht there is some basic clusters where all customer order items and there is some other nearly empty (for example cluster 76). If we have to orient our company, we could reduce the order of this type of parts and focus on main buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad997270795573a46a5a225d53803c9e89bc0a85"
   },
   "source": [
    "# Building Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cadfe6c911aa9c7841ca88bc9c061d13afdcc0d9"
   },
   "source": [
    "We saw that all classes are quite different. For a balance in production, we can guess that a Tree model should work well. At first, we will try it and based on the result, we may change to another model. There is also 2 additionnal good point for this model, we can build the tree an see visually the outcome. But another advantage, is to be able to use raw data without scaling. The evaluation will be done on Cross Validation as we don't have a lot of  customer to also keep a validation dataset (al least for nearly empty clusters). As the smallest cluster has 6 customer, we will use 5 split for the validation and we will track accuracy and the standard deviation between all score. Which means that it's not really repetible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "_uuid": "12270c25f4fca51b15eb84c6e2787f1201705008"
   },
   "outputs": [],
   "source": [
    "clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "_uuid": "dc88d8c5e35e1567e74a27b6a423e29fa5dff381"
   },
   "outputs": [],
   "source": [
    "classification_dataset = pd.concat([df_grouped_final, df2_grouped_final], axis = 1)\n",
    "classification_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "_uuid": "87459dc3e95dd9a42c2adedf59da08996f012977",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = classification_dataset.drop(\"cluster\", axis=1).as_matrix()\n",
    "y = classification_dataset[\"cluster\"].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "_uuid": "c24f07d29563533face16e7da9f256d3fc0c6c64",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_samples_split = 4 \n",
    "cv = 5\n",
    "n_estimators = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "_uuid": "dc5fa2e1c817470525d0e878bb9c72c20ae5f9c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "_uuid": "d7ba9c360e25e00ef42f1b82963172c9553076ca"
   },
   "outputs": [],
   "source": [
    "for max_depth in range(3, 10):\n",
    "    clf = DecisionTreeClassifier(random_state=0, max_depth=max_depth, min_samples_split=min_samples_split )\n",
    "    scores = cross_val_score(estimator=clf, X=X, y=y, cv=5, n_jobs=8)\n",
    "    scores = np.array(scores)\n",
    "    print(\"Depth {} : Acc {:.3f} - Dev {:.3f}\".format(max_depth, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "_uuid": "ebcbad02b641d06be4d24fcf90deabb102a73262"
   },
   "outputs": [],
   "source": [
    "for max_depth in range(3, 10):\n",
    "    clf = ExtraTreeClassifier(random_state=0, max_depth=max_depth, min_samples_split=min_samples_split )\n",
    "    scores = cross_val_score(estimator=clf, X=X, y=y, cv=cv, n_jobs=8)\n",
    "    scores = np.array(scores)\n",
    "    print(\"Depth {} : Acc {:.3f} - Dev {:.3f}\".format(max_depth, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "_uuid": "85ca9b708ccf37d3af93d4e659cd9efce7586951"
   },
   "outputs": [],
   "source": [
    "for max_depth in range(3, 10):\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=0, max_depth=max_depth, min_samples_split=min_samples_split )\n",
    "    scores = cross_val_score(estimator=clf, X=X, y=y, cv=cv, n_jobs=8)\n",
    "    scores = np.array(scores)\n",
    "    print(\"Depth {} : Acc {:.3f} - Dev {:.3f}\".format(max_depth, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "_uuid": "7193920db53a8cee5c545bcfd41aa588765d5eed"
   },
   "outputs": [],
   "source": [
    "for max_depth in range(3, 10):\n",
    "    clf = ExtraTreesClassifier(n_estimators=n_estimators, random_state=0, max_depth=max_depth, min_samples_split=min_samples_split )\n",
    "    scores = cross_val_score(estimator=clf, X=X, y=y, cv=cv, n_jobs=8)\n",
    "    scores = np.array(scores)\n",
    "    print(\"Depth {} : Acc {:.3f} - Dev {:.3f}\".format(max_depth, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c874619090e21418814d2454e4c2486cec4b7bd"
   },
   "source": [
    "The DecisionTreeClassifier is the best in accuracy but the Std Dev is higher than the RandomForestClassifier which have a smaller accuracy. Models usign ExtraTrees are not working as good as Random one so we will exclude them.\n",
    "At first sight, I'd keep the RandomForestClassifier as we have the benefit to avoid overfitting due to the ensemble of classifier. Let see the confusion matrice for both models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "_uuid": "9a9a98b26dbd5471df0d2c02bfc4a3459366ba02"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "clf1 = DecisionTreeClassifier(random_state=0, max_depth=9, min_samples_split=4 )\n",
    "clf2 = RandomForestClassifier(random_state=0, max_depth=9, min_samples_split=4, n_estimators=n_estimators)\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)\n",
    "y_pred2 = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "_uuid": "79415bdfe9220c52487139562564bc360f9c4d36"
   },
   "outputs": [],
   "source": [
    "np.unique(y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "_uuid": "5e43f8dcc37121172562093c5a3e2a99c3006560"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "#     plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "classes = [\"Cluster {}\".format(x) for x in range(6)]\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.subplot(1, 2, 1)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred1)\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, title='Confusion matrix, with DecisionTreeClassifier')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes, title='Confusion matrix, with RandomForestClassifier')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73c149f32b9b677f07cf6025a2362fd374316549"
   },
   "source": [
    "So we trained the model with fixed values and a splitted dataset 2/3 for train and 1/3 to test. We can see that there is clearly more mistakes with ensemble learning and not with Random Forest. Moreover, in addition of having more accuracy, it also classify properly the cluster 4 with our VIP customer which is a good point. The main error is between cluster 0 and 1 vs Cluster 5. The main difference is that is some cases we lost the customer. This can be analysed in a good point too. As we classify some customer lost as customer not ofter active, that may mean that they are borderline and we may be able to convert them to permanent customer with some actions. As a result, This model can be used in production\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this notebook, I tried to explain as much as possible to work done to segment customer based on their behavior. We succeed to find some specific patterns and create a model able to classify them from some of their order. We can also use this work to drive some actions like items to stop purchaing, trending items, detect customer being lost, rewoard our VIP customer and so on. \n",
    "\n",
    "I hope you liked it  and feel free to let me know if you have comments or improvements to do on this dataset !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "618e98d2b9b6c082e2d099f42658aceb6d4cb74c",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
